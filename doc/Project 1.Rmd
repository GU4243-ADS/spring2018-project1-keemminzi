---
title: "Project 1 SPOOKY text analysis"
author: "Minzi Keem"
date: "2/5/2018"
output: html_document
---

# Section 0: check and install necessary packages, load the libraries and the functions.

## Setup the libraries
```{r, message = F, warning = F}
packages.used <- c("ggplot2", "dplyr", "tibble", "tidyr",  "stringr", "tidytext", "topicmodels", "wordcloud", "ggridges", "igraph", "ggraph", "forcats")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

# install additional packages
if(length(packages.needed) > 0) {
  install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}

library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
library(igraph)
library(ggraph)
library(forcats)

source("../libs/multiplot.R")
```

## Read in the data
```{r}
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
```

## An overview of the data structure and content
Let's first remind ourselves of the structure of the data.
```{r}
head(spooky)
summary(spooky)
```

# Section 1: Data cleaning
We first use the `unnest_tokens()` function to drop all punctuation and transform all words into lower case from the `tidytext` package. `tidytext` also contains stop words that we will eliminate. 
```{r}
# Makes a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
```

# Section 2: Word Frequency

## Wordcloud - all
We check to see the word frequency for the fifty most common words, and visualize them in a wordcloud.
```{r, include = FALSE}
# Words is a list of words, and freqs their frequencies
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n

png("../figs/Worldcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()
```
This wordcloud shows that "time", "night", "found", "life", "day" and "eyes" appear often between all of the texts.

## Wordcloud - separated by author
We check to see the word frequency for the fifty most common words, separated by author. First, for Edgar Allen Poe, then HP Lovecraft, then Mary Shelley.
```{r, include = FALSE}
#EAP
# Words is a list of words, and freqs their frequencies
EAP <- filter(spooky_wrd, author == "EAP")
words <- count(group_by(EAP, word))$word
freqs <- count(group_by(EAP, word))$n

png("../figs/Worldcloud_EAP.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4"))
dev.off()

#HPL
# Words is a list of words, and freqs their frequencies
HPL <- filter(spooky_wrd, author == "HPL")
words <- count(group_by(HPL, word))$word
freqs <- count(group_by(HPL, word))$n

png("../figs/Worldcloud_HPL.png")
wordcloud(words, freqs, max.words = 50, color = c("red4"))
dev.off()

#MWS
# Words is a list of words, and freqs their frequencies
MWS <- filter(spooky_wrd, author == "MWS")
words <- count(group_by(MWS, word))$word
freqs <- count(group_by(MWS, word))$n

png("../figs/Worldcloud_MWS.png")
wordcloud(words, freqs, max.words = 50, color = c("black"))
dev.off()
```

# Section 3: Punctuations
## Count the number of commas, hyphens, exclamation points
```{r}
EAP_sentence <- filter(spooky, author == "EAP")
EAP_sentence$comma <- str_count(EAP_sentence$text, ',')
EAP_sentence$colon <- str_count(EAP_sentence$text, '[:]')
EAP_sentence$quot <- str_count(EAP_sentence$text, '["]')
EAP_sentence$semi <- str_count(EAP_sentence$text, ';')
EAP_sentence$question <- str_count(EAP_sentence$text, '[?]')

HPL_sentence <- filter(spooky, author == "HPL")
HPL_sentence$comma <- str_count(HPL_sentence$text, ',')
HPL_sentence$colon <- str_count(HPL_sentence$text, '[:]')
HPL_sentence$quot <- str_count(HPL_sentence$text, '["]')
HPL_sentence$semi <- str_count(HPL_sentence$text, ';')
HPL_sentence$question <- str_count(HPL_sentence$text, '[?]')

MWS_sentence <- filter(spooky, author == "MWS")
MWS_sentence$comma <- str_count(MWS_sentence$text, ',')
MWS_sentence$colon <- str_count(MWS_sentence$text, '[:]')
MWS_sentence$quot <- str_count(MWS_sentence$text, '["]')
MWS_sentence$semi <- str_count(MWS_sentence$text, ';')
MWS_sentence$question <- str_count(MWS_sentence$text, '[?]')
```

## Graphically represent our findings of punctuation counts
```{r, include = FALSE}
punctuation <- rep(c("commas", "colons", "quotation marks", "semicolons", "question marks"), 3)
p1 <- c(sum(EAP_sentence$comma), sum(EAP_sentence$colon), sum(EAP_sentence$quot), sum(EAP_sentence$semi), sum(EAP_sentence$question))
p2 <- c(sum(HPL_sentence$comma), sum(HPL_sentence$colon), sum(HPL_sentence$quot), sum(HPL_sentence$semi), sum(HPL_sentence$question))
p3 <- c(sum(MWS_sentence$comma), sum(MWS_sentence$colon), sum(MWS_sentence$quot), sum(MWS_sentence$semi), sum(MWS_sentence$question))
values <- c(p1, p2, p3)
author <- c(rep("EAP", 5), rep("HPL", 5), rep("MWS", 5))
df <- data.frame(punctuation, values, author)

p <- ggplot() + geom_bar(aes(y = values, x = author, fill = punctuation), data = df, stat="identity") +
    scale_fill_hue(c=45, l=80)
ggsave("Punctuation_Count.png", plot = last_plot(), device = "png", path = "../figs/")
```

## Graphically represent averages per sentence 
```{r, include = FALSE}
p1 <- p1/length(EAP_sentence$text)
p2 <- p2/length(HPL_sentence$text)
p3 <- p3/length(MWS_sentence$text)
values <- c(p1, p2, p3)
author <- c(rep("EAP", 5), rep("HPL", 5), rep("MWS", 5))
df <- data.frame(punctuation, values, author)

p2 <- ggplot() + geom_bar(aes(y = values, x = author, fill = punctuation), data = df, stat="identity", position = "dodge") + scale_fill_hue(c=45, l=80)
ggsave("Punctuation_Averages.png", plot = last_plot(), device = "png", path = "../figs/")
```

# Section 4: Bigrams
```{r}
temp <- select(spooky, author, text)
temp <- unnest_tokens(temp, bigram, text, token = "ngrams", n = 2)

#separate the bigrams and attach them again
bigram_sep <- separate(temp, bigram, c("word1", "word2"), sep = " ")
temp2 <- filter(bigram_sep, !word1 %in% stop_words$word)
temp2 <- filter(temp2, !word2 %in% stop_words$word)

#attach back together after removing stop words
temp <- unite(temp2, bigram, word1, word2, sep = " ")

#extract tf-idf values
temp3 <- count(temp, author, bigram)
temp3 <- bind_tf_idf(temp3, bigram, author, n)
test_tf_idf <- arrange(temp3, desc(tf_idf))
```

## Graphically represent results
```{r, include = FALSE}
a <- arrange(test_tf_idf, desc(tf_idf))
a <- mutate(a, bigram = factor(bigram, levels = rev(unique(bigram))))
a <- group_by(a, author)
a <- top_n(a, 10, tf_idf)
a <- ungroup(a) 
ggplot(a, aes(bigram, tf_idf, fill = author)) + geom_col() + labs(x = NULL, y = "TF-IDF values") +
  theme(legend.position = "none") + facet_wrap(~ author, ncol = 3, scales = "free") + coord_flip()
ggsave("Bigram_tf-idf.png", plot = last_plot(), device = "png", path = "../figs/")
```

## Networks of bigrams
```{r}
bigram_counts <- count(temp2, word1, word2, sort = TRUE)
b <- filter(bigram_counts, n > 6)
bigram_graph <- graph_from_data_frame(b)

set.seed(1234)

a <- grid::arrow(type = "closed", length = unit(.1, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "#FF9999", size = 3) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

ggsave("Bigram_network.png", plot = last_plot(), device = "png", path = "../figs/")
```

# Section 5: Gendered Words
```{r}
a <- unnest_tokens(spooky, word, text)
a <- filter(a, (word == "man") | (word == "woman"))
a <- mutate(a, word = as.factor(word))
mwcomp <- ggplot(a, aes(word, fill = author)) + geom_bar(position = "dodge") + scale_fill_hue(c=70, l=80)

b <- unnest_tokens(spooky, word, text)
b <- filter(b, (word == "he") | (word == "she"))
b <- mutate(b, word = as.factor(word))
heshe <- ggplot(b, aes(word, fill = author)) + geom_bar(position = "dodge") + scale_fill_hue(c=70, l=80)

c <- unnest_tokens(spooky, word, text)
c <- filter(c, (word == "him") | (word == "her"))
c <- mutate(c, word = fct_relevel(word, "him", "her"))
himher <- ggplot(c, aes(word, fill = author)) + geom_bar(position = "dodge") + scale_fill_hue(c=70, l=80)

d <- unnest_tokens(spooky, word, text)
d <- mutate(d, male = ( word == "he" | word == "him" | word == "his" | word == "male" |
                    word == "man" | word == "gentleman" | word == "sir" |
                    word == "lord" | word == "men" ))
d <- mutate(d, female = ( word == "she" | word == "her" | word == "hers" | word == "female" |
                    word == "woman" | word == "lady" | word == "madam" |
                    word == "women" ))
d <- unite(d, sex, male, female)
d <- mutate(d, word = fct_relevel(word, "male", "female")) #note, I tried to relevel the words so that they're in order but I don't think this code is working for some reason..
d <- mutate(d, sex = fct_recode(as.factor(sex), male = "TRUE_FALSE", 
                          female = "FALSE_TRUE", other = "FALSE_FALSE"))
d <- filter(d, sex != "other")

fmcomp <- ggplot(d, aes(sex, fill = author)) + labs(x = "Gender indicators") + geom_bar(position = "dodge") + scale_fill_hue(c=70, l=80)
  
layout <- matrix(c(1,2,3,4), 2, 2, byrow = TRUE)
multiplot(mwcomp, heshe, himher, fmcomp, layout = layout)
ggsave("Gender_comp.png", plot = last_plot(), device = "png", path = "../figs/")

```










